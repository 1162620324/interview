[TOC]

## 数学知识
#### 1.均匀分布
 ![](.\图片\均匀分布.png "")

#### 2.二项分布
 ![](.\图片\二项分布1.png "")
 ![](.\图片\二项分布2.png "")
 ![](.\图片\二项分布的协方差.png "")

#### 3.高斯分布
 ![](.\图片\一维正态分布.png "")
 ![](.\图片\二维正态分布.png "")

#### 4.泊松分布
 ![](.\图片\泊松分布1.png "")
 ![](.\图片\泊松分布2.png "")

#### 5.指数分布
 ![](.\图片\指数分布.png "")
 ![](.\图片\指数分布1.png "")
 ![](.\图片\指数分布2.png "")
 ![](.\图片\指数分布3.png "")

#### 6.几何分布
 ![](.\图片\几何分布1.png "")
 ![](.\图片\几何分布2.png "")
 

## 损失函数
 #### 1.交叉熵损失函数：
 ![](.\图片\CE.png "")




## [Nomalization（归一化）](https://zhuanlan.zhihu.com/p/33173246)
#### 1. [Transformer自然语言处理为甚么使用LN](https://www.zhihu.com/question/395811291/answer/1251829041)
![Transformer自然语言处理为甚么使用LN](.\图片\归一化1.png "Transformer自然语言处理为甚么使用LN")
#### 2.为什么需要normalization
* 独立同分布与白化
* 深度神经网络涉及到很多层的叠加，而每一层的参数更新会导致上层的输入数据分布发生变化，通过层层叠加，高层的输入分布变化会非常剧烈，这就使得高层需要不断去重新适应底层的参数更新。为了训好模型，我们需要非常谨慎地去设定学习率、初始化权重、以及尽可能细致的参数更新策略。
* 协变量偏移 ![](.\图片\协变量偏移.png "")
*  - 上层参数需要不断适应新的输入数据分布，降低学习速度
   - 下层输入的变化可能趋向于变大或者变小，导致上层落入饱和区，使得学习过早停止
   - 每层的更新都会影响到其它层，因此每层的参数更新策略需要尽可能的谨慎
#### 3.Normalization 的通用框架与基本思想
* 要解决独立同分布的问题，“理论正确”的方法就是对每一层的数据都进行白化操作。然而标准的白化操作代价高昂，特别是我们还希望白化操作是可微的，保证白化操作可以通过反向传播来更新梯度
* 通用变换框架：（***为了保证模型的表达能力不因为规范化而下降*。**）
  ![](.\图片\norm_框架.png "")
  - 将规范化后的数据进行再平移和再缩放，使得每个神经元对应的输入范围是针对该神经元量身定制的一个确定范围。**rescale** 和 **reshift** 的参数都是可学习的，这就使得 Normalization 层可以学习如何去尊重底层的学习结果。
  - 另一方面的重要意义在于保证获得非线性的表达能力
#### 4. 主流 Normalization 方法梳理
* **Batch Normalization —— 纵向规范化**
  - **每个 mini-batch 比较大，数据分布比较接近。在进行训练之前，要做好充分的 shuffle. 否则效果会差很多。**
   ![](.\图片\BN.png "")

* **Layer Normalization —— 横向规范化**
  - **优点：** LN 针对单个训练样本进行，不依赖于其他数据，因此可以避免 BN 中受 mini-batch 数据分布影响的问题，可以用于 小mini-batch场景、动态网络场景和 RNN，特别是自然语言处理领域。此外，LN 不需要保存 mini-batch 的均值和方差，节省了额外的存储空间。
  - **缺点：** BN 的转换是针对单个神经元可训练的——不同神经元的输入经过再平移和再缩放后分布在不同的区间，而 LN 对于一整层的神经元训练得到同一个转换——所有的输入都在同一个区间范围内。如果**不同输入特征不属于相似的类别（比如颜色和大小）**，那么 LN 的处理可能会降低模型的表达能力。
  ![](.\图片\LN.png "")

* **Weight Normalization —— 参数规范化**
  - BN 和 LN 均将规范化应用于输入的特征数据 **X** ，而 WN 则另辟蹊径，将规范化应用于线性变换函数的权重 ***W*** ，这就是 WN 名称的来源。
  - 另外，我们看到这里的规范化只是对数据进行了 scale，而没有进行 shift，因为我们简单地令 **u = 0**. 但事实上，这里留下了与 BN 或者 LN 相结合的余地——那就是利用 BN 或者 LN 的方法来计算输入数据的均值 。
  - WN 的规范化不直接使用输入数据的统计量，因此避免了 BN 过于依赖 mini-batch 的不足，以及 LN 每层唯一转换器的限制，同时也可以用于动态网络结构。
  
#### 4. Normalization 为什么会有效？
  - **Normalization 权重伸缩不变性**
    权重伸缩不变性可以有效地提高反向传播的效率。
    权重伸缩不变性还具有参数正则化的效果，可以使用更高的学习率。
  - **Normalization 数据伸缩不变性**
    数据伸缩不变性可以有效地减少梯度弥散，简化对学习率的选择。


## 反向传播算法


## 优化算法(SGD, SGDM,NAG,AdaGrad...)
><font size = 5>定义</font>：
![](.\图片\优化.png "")
注意：用到一阶动量的方法：SGD,SGDM,SGD with nesterov
用到二阶动量的方法：AdaGrad, AdaDelta, Adam
### 1.SGD
![](.\图片\SGD.png "")
### 2.SGDM
![](.\图片\SGDM.png "")
### 3.SGD with Nesterov
![](.\图片\SGD_with_Nesterov.png "")
### 4.AdaGrad
* 优点：使用二阶动量，自适应学习率
* 缺点：sqart(V)是单调递减的，学习率可能会递减至0，训练提前结束
  
![](.\图片\AdaGrad.png "")
### 5.AdaDelta / RMSProp
![](.\图片\Rmsprop_AdaDelta.png "")
### 6.Adam
* 缺点：
  随着时间窗口的变化，遇到的数据可能发生巨变，使得 [公式] 可能会时大时小，不是单调变化。这就可能在训练后期引起学习率的震荡，导致模型无法收敛
  自适应学习率算法可能会对前期出现的特征过拟合，后期才出现的特征很难纠正前期的拟合效果。
  
![](.\图片\Adam.png "")
![](.\图片\Adam改进.png "")


## 神经网络参数初始化
#### 1.初始化类型：
* **固定值初始化：** 指将模型参数初始化为一个固定的常数，这意味着所有单元具有相同的初始化状态，所有的神经元都具有相同的输出和更新梯度，并进行完全相同的更新，这种初始化方法使得神经元间不存在非对称性，从而使得模型效果大打折扣。
* **预训练初始化：** 神经网络初始化的有效方式，比较早期的方法是使用 greedy layerwise auto-encoder 做无监督学习的预训练，经典代表为 Deep Belief Network；而现在更为常见的是有监督的预训练+模型微调。
* **随机初始化：** 随机进行参数初始化，但如果不考虑随机初始化的分布则会导致梯度爆炸和梯度消失的问题。
  
#### 2.随机初始化：
* **1.Naive Initialization：** 
    ![](.\图片\初始化1.png "")
    ![](.\图片\初始化2.png "")

* **2.Xavier Initialization：**
    >Xavier Glorot 认为：优秀的初始化应该使得各层的激活值和状态梯度在传播过程中的方差保持一致。即**方差一致性**。
    所以我们需要同时考虑正向传播和反向传播的输入输出的方差相同。
    在开始推导之前，我们先引入一些必要的假设：
    1.x、w、b 相同独立；
    2.各层的权重 w 独立同分布，且均值为 0；
    3.偏置项 b 独立同分布，且方差为 0；
    4.输入项 x 独立同分布，且均值为 0；


    * **Forward**
    ![](.\图片\X初始化.png "")
    * **Backward** 
    ![](.\图片\X初始化2.png "")
    * **为什么Xavier初始化输出方差不是1？**
      > 这是因为 sigmoid 的输出都为正数，所以会影响到均值的分布，所以会导致下一层的输入不满足均值为 0 的条件。这会导致Zigzag现象：
      ![](.\图片\Zigzag.png "")
      ![](.\图片\Zigzag1.png "")
      **Xavier初始化失效** ![](.\图片\Xavier初始化失效.png "")

* **2.He Initialization：**
    ![](.\图片\He初始化.png "")
    


## BERT常见问题
### 模型架构问题
#### 1.为什么attention is all you need中要乘上sqrt(hidden)？?
> **Answer:** 因为位置编码使用sin/cos，值域比xavier初始化的token embedding大很多[-0.13,0.13]，所以要增强。但是BERT改变了position embedding的方式，就没有必要了
> ![](.\图片\Transformer初始化.png "")

#### 2.为什么是缩放点积，而不是点积模型？
> **Answer:** 当输入信息的维度 d 比较高，点积模型的值通常有比较大方差，从而导致 softmax 函数的梯度会比较小。因此，缩放点积模型可以较好地解决这一问题。
    ![](.\图片\缩放点积.png "")
    ![](.\图片\缩放点积1.png "")
    ![](.\图片\缩放点积2.png "")

#### 3.相较于加性模型，点积模型具备哪些优点？
>**Answer:** 常用的Attention机制为加性模型和点积模型，理论上加性模型和点积模型的复杂度差不多，但是点积模型在实现上可以更好地利用矩阵乘积，从而计算效率更高（实际上，随着维度d的增大，加性模型会明显好于点积模型）。

#### 4.为什么是双线性点积模型？
>**Answer:** 使用Q/K/V不相同可以保证在不同空间进行投影，增强了表达能力，提高了泛化能力。
双线性点积模型，引入非对称性，更具健壮性（Attention mask对角元素值不一定是最大的，也就是说当前位置对自身的注意力得分不一定最高）。
![](.\图片\Attention.png "")

#### 5.多头机制为什么有效？
>**Answer:** **1.** 类似于CNN中通过多通道机制进行特征选择；**2.** Transformer中先通过切头（spilt）再分别进行Scaled Dot-Product Attention，可以使进行点积计算的维度d不大（防止梯度消失），同时缩小attention mask矩阵。
![](.\图片\MHA.png "")

#### 6.FFN的作用？
>**Answer:** Transformer在抛弃了 LSTM 结构后，FFN 中的 ReLU成为了一个主要的提供非线性变换的单元。 通过将向量先放缩至高维空间，然后通过Relu进行特征筛选，再放缩回来。

#### 7.权重共享
>**Answer:**  词表数量级高，这样可以减少参数量
虽然weight共享了，但是embedding和pre-softmax仍然是两个不同的层，因为bias是彼此独立的。
![](.\图片\Weight_Tying.png "")

#### 8.BERT激活函数Gelu
>**Answer:** 在激活中引入了随机正则的思想，
根据当前input大于其余inputs的概率进行随机正则化，即为在mask时依赖输入的数据分布，即x越小越有可能被mask掉，因此服从bernoulli(Φ(x))。**相较于Relu**缺乏随机因素，只用0和1。
![](.\图片\GELU1.png "")
![](.\图片\GELU2.png "")

#### 9.BERT的优缺点
>**Answer:** 
> **优点:** **1.** 利用了文本的双向信息，得到基于上下文的token表示，效果更好。**2.** 计算可并行化。
> **缺点:** **1.** [MASK]标记在实际预测中不会出现，训练时用过多[MASK]影响模型表现。 **2.** 每个batch只有15%的token被预测，所以BERT收敛得比left-to-right模型要慢（它们会预测每个token）。**3.**不适合生成式任务

### NLU训练方法
#### 1. Subword (BERT)
 * 15% tokens: 80% [MASK], 10% random, 10% unchanged。在预处理阶段，给每个句子采样10种mask方式 
#### 2. Word (WWM)
 根据分词后的词边界mask
#### 3. Entity/Phrase (Baidu ERNIE1.0) 
  * word-level/phrase-leve/entity-level。 50%的时候选entity或phrase，剩下选word（保持总体subword在15%）
  [ernie1.0](https://blog.csdn.net/doyouseeman/article/details/113835522)
  ![](.\图片\ernie1.png "")
  ![](.\图片\ernie2.png "")
  ![](.\图片\ernie3.png "")
  ![](.\图片\ernie4.png "")
  [ernie2.0](https://blog.csdn.net/ljp1919/article/details/100556907)
  ![](.\图片\ernie2.01.png "")
  ![](.\图片\ernie2.02.png "")
  ![](.\图片\ernie2.03.png "")
  ![](.\图片\ernie2.04.png "")
#### 4. [Span/N-gram (SpanBERT)](https://zhuanlan.zhihu.com/p/75893972) 
* 根据几何分布，先随机选择一段（span）的长度，之后再根据均匀分布随机选择这一段的起始位置，最后按照长度遮盖。文中使用几何分布取 p=0.2，最大长度只能是 10，平均被遮盖长度是 3.8 个词的长度。
参考Roberta的动态masking/一直训练长句
  ![](.\图片\spanBert1.png "")
  ![](.\图片\spanBert3.png "")
  **为什么NSP任务没有用**
  ![](.\图片\spanBert2.png "")

#### 5. Dynamic (RoBERTa)
* 每个Epoch见到的样本mask位置都不一样，实际上效果只提升了不到一个点
  
#### 6.PLM(XLnet)
> ![](.\图片\XLnet3.png "")
 ![](.\图片\XLnet4.png "")




## 预训练模型
### [XLnet](https://zhuanlan.zhihu.com/p/70257427)
 > **基本思想内容：**![](.\图片\XLnet1.png "")
 ![](.\图片\XLnet2.png "")
 ![](.\图片\XLnet3.png "")
 ![](.\图片\XLnet4.png "")
#### [双流自注意力](https://zhuanlan.zhihu.com/p/107560878)
 ![](.\图片\双流自注意力2.png "")
 ![](.\图片\双流自注意力1.png "")
#### [Transformer-XL](https://www.zhihu.com/search?type=content&q=TRansformer-XL)



## 广告相关模型
### 1.传统CTR预估模型
#### 1) LR模型
 * 定义
 ![](.\图片\CTR_LR1.png "")
 * 数据
 ![](.\图片\CTR_LR2.png "")
 ![](.\图片\CTR_LR3.png "")
 ![](.\图片\CTR_LR4.png "")
 ![](.\图片\CTR_LR5.png "")
 * 模型
 ![](.\图片\ctr_LR6.png "")
 ![](.\图片\CTR_LR7.png "")

#### 2) FM模型
 * 相关定义：
 ![](.\图片\CTR_FM.png "")
 * 模型：
 ![](.\图片\CTR_FM1.png "")
 ![](.\图片\CTR_FM2.png "")
 ![](.\图片\CTR_FM3.png "")
 ![](.\图片\CTR_FM4.png "")
 ![](.\图片\CTR_FM5.png "")
 ![](.\图片\CTR_FM6.png "")
 ![](.\图片\CTR_FM7.png "")
 ![](.\图片\CTR_FM8.png "")

#### 3) FFM模型
 * 模型：
 ![](.\图片\CTR_FFM1.png "")
 ![](.\图片\CTR_FFM2.png "")
 ![](.\图片\CTR_FFM3.png "")
 ![](.\图片\CTR_FFM4.png "")

#### 4) GBDT_LR模型
 * 模型
 ![](.\图片\GBDT_LR1.png "")
 ![](.\图片\GBDT_LR2.png "")

### 2.神经网络CTR预估模型
#### 1) DSSM
 * 问题：
 ![](.\图片\CTR_DSSM.png "")
 * 模型：
 ![](.\图片\CTR_DSSM1.png "")
 ![](.\图片\ctr_dssm2.png "")
 ![](.\图片\CTR_DSSM3.png "")
 ![](.\图片\CTR_DSSM4.png "")
 * word_hash技术
 ![](.\图片\CTR_DSSM5.png "")

#### 2) FNN
 * 问题及思路：
  ![](.\图片\CTR_fnn1.png "")
 * 模型：
 ![](.\图片\CTR_fnn2.png "")
 ![](.\图片\CTR_fnn3.png "")
 ![](.\图片\CTR_fnn4.png "")

#### 3) Wide&Deep
 * 问题及思路：
 ![](.\图片\WD1.png "")
 > 即：广义线性模型表达能力不强，容易欠拟合；深度神经网络模型表达能力太强，容易过拟合。二者结合就能取得平衡。

 * 模型：
 ![](.\图片\WD2.png "")
 ![](.\图片\WD4.png "")
 ![](.\图片\WD5.png "")
   

## 项目并行开发管理
  ![](.\图片\项目并行管理1.png "")
  ![](.\图片\项目并行管理2.png "")
  ![](.\图片\项目并行管理3.png "")
  ![](.\图片\项目并行管理.png "")
  ![](.\图片\项目并行管理4.png "")

## 图神经网络

## [推荐相关机器学习XgBoost, GBDT...](http://www.huaxiaozhuan.com/%E7%BB%9F%E8%AE%A1%E5%AD%A6%E4%B9%A0/chapters/7_GBT.html)
### GBT
 **基本思路** 
 > ![](.\图片\GBT1.png "")
 **算法**
 > ![](.\图片\GBT2.png "")
#### GBDT
 **模型**
 > ![](.\图片\GBT3.png "")
 > ![](.\图片\GBT4.png "")
 **正则化**
 > ![](.\图片\GBT5.png "")
 **RF vs GBT**
 > ![](.\图片\GBT6.png "")

 #### XgBoost
 **模型**
 > ![](.\图片\xgboost1.png "")
 > ![](.\图片\xgboost2.png "")
 **结构分**
 > ![](.\图片\xgboost3.png "")
 > ![](.\图片\xgboost4.png "")












